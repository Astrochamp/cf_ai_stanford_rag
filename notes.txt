1. fetch SEP list of articles - https://plato.stanford.edu/published.html
2. check changes - https://plato.stanford.edu/new.html or https://plato.stanford.edu/rss/sep.xml
3. fetch new articles and revised articles
4. parse articles & update metadata in db
5. chunk articles & save chunks + metadata
6. vectorise and normalise chunks, attach metadata, upload to Vectorize

- chunk first, then preprocess
- store two versions: cleaned (retrieval), rich (user display + LLM generation)

https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/rpc/
https://github.com/niieani/gpt-tokenizer

Pipeline:
Query -> embed (dense) -> dense search -> 50 chunks
Query -> embed (sparse) -> sparse search (BM25) -> 50 chunks
RRF: dense & sparse -> 25 chunks
BGE: 25 -> 5 (use cleaned)
LLM generation: 5 parent sections in rich format

Chunk size: <= 1024 (semantic chunking)
BM25: b=0.85, k1=1.6

