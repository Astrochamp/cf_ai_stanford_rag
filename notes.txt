1. fetch SEP list of articles - https://plato.stanford.edu/published.html
2. check changes - https://plato.stanford.edu/new.html or https://plato.stanford.edu/rss/sep.xml
3. fetch new articles and revised articles
4. parse articles & update metadata in db
5. chunk articles & save chunks + metadata
6. vectorise and normalise chunks, attach metadata, upload to Vectorize

- convert href
- remove images if present
- remove tables
- strip all html tags and keep content only
- parse html entities e.g. &quot; , &rsquo;
- keep tex

<strong>Sakya Pa &#7751;&#7693;ita [sa skya pa &#7751;&#7693;i ta]</strong>
search for " &#" and remove space
store as ascii with html entities, use article URL as ID (sakya-pandita)
to normalise:
```
const originalText = "François, the naïve protégé from a château in Besançon, enjoyed a piña colada in Zürich with his fiancée, Maëlle.";

// Normalize to NFD, then use a regex to remove combining diacritical marks
const normalizedText = originalText
  .normalize('NFD')
  .replace(/[\u0300-\u036f]/g, '');
```



To rewrite TeX (<TEXT>):
```
Convert TeX expressions in this text to concise plain English. Respond with the updated text and nothing else, WITHOUT triple-quotes. Do NOT think step-by-step. Do NOT make any other changes to the text.

Text:
"""
<TEXT>
"""
```


To reformat HTML tables (<TABLE>, <ARTICLE>, <SECTION>):
```
Write a concise natural language description of this HTML table. The table may contain TeX expressions. Respond with the description and nothing else, WITHOUT triple-quotes. Do NOT think step-by-step. Context: SEP, <ARTICLE>, <SECTION>

Table:
"""
<TABLE>
"""
```