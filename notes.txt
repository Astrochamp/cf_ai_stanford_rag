1. fetch SEP list of articles - https://plato.stanford.edu/published.html
2. check changes - https://plato.stanford.edu/new.html or https://plato.stanford.edu/rss/sep.xml
3. fetch new articles and revised articles
4. parse articles & update metadata in db
5. chunk articles & save chunks + metadata
6. vectorise and normalise chunks, attach metadata, upload to Vectorize

- chunk first, then preprocess
- store two versions: cleaned (retrieval), rich (user display + LLM generation)

https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/rpc/
https://github.com/niieani/gpt-tokenizer

Pipeline:
Query -> embed (dense) -> dense search -> 50 chunks
Query -> embed (sparse) -> sparse search (BM25) -> 50 chunks
RRF: dense & sparse -> 25 chunks
BGE: 25 -> 5 (use cleaned)
LLM generation: 5 parent sections in rich format

Chunk size: <= 1024 (semantic chunking)
BM25: b=0.85, k1=1.6

===

SEP Oracle

initial setup, per article:
1. fetch article
2. preprocess and chunk (two formats: retrieval & generation)
3. store retrieval format in D1
4. store generation format in R2
5. vectorise retrieval format chunks & store in Vectorize

to answer user query:
1. query Vectorize
2. embed query and search D1 with BM25
3. unify top-K with RRF and rerank
4. dedupe chunks then grab top N chunks + their immediate neighbours (max ~8k tokens)