1. fetch SEP list of articles - https://plato.stanford.edu/published.html
2. check changes - https://plato.stanford.edu/new.html or https://plato.stanford.edu/rss/sep.xml
3. fetch new articles and revised articles
4. parse articles & update metadata in db
5. chunk articles & save chunks + metadata
6. vectorise and normalise chunks, attach metadata, upload to Vectorize

- chunk first, then preprocess
- store two versions: cleaned (retrieval), rich (user display + LLM generation)

https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/rpc/
https://github.com/niieani/gpt-tokenizer

Pipeline:
Query -> embed (dense) -> dense search -> 50 chunks
Query -> embed (sparse) -> sparse search (BM25) -> 50 chunks
RRF: dense & sparse -> 25 chunks
BGE: 25 -> 5 (use cleaned)
LLM generation: 5 parent sections in rich format

Chunk size: <= 1024 (semantic chunking)
BM25: b=0.85, k1=1.6

===

SEP Oracle

initial setup, per article:
1. fetch article
2. preprocess and chunk (two formats: retrieval & generation)
3. store generation format in R2
4. store retrieval format in D1
5. vectorise retrieval format chunks & store in Vectorize

to answer user query:
1. query Vectorize
2. embed query and search D1 with BM25
3. unify top-K with RRF and rerank
4. dedupe chunks then grab top N chunks + their immediate neighbours (max ~8k tokens)




system prompt:
```
<System>
You are an expert philosopher and evidence-first summariser. GUIDELINES (must follow exactly):
- Use ONLY the passages provided in the "EVIDENCE" array below to support factual claims.
- For every factual claim, append a parenthetical citation matching the chunk id exactly, e.g. (propositional-logic/3.1/0).
- For each claim that is NOT supported by provided passages, explicitly label it: "UNSUPPORTED BY PROVIDED SOURCES".
- Produce output in British English. Use British spellings (e.g., 'analyse', 'colour', 'organisation'), British punctuation conventions and date format DD/MM/YYYY.
- When quoting a passage verbatim, use single quotation marks unless nested quotes require double quotes. Always include the chunk id after the quote.
- Keep answer focused, precise, and avoid speculative claims beyond the evidence.
- Output two JSON keys: "answer" (string) and "used_evidence" (array of objects: {id, verbatim_quote, role_in_answer}).
- If asked for further reading, list the doc_title and section_heading for cited chunks only.

Default behaviour:
- reasoning_effort: MEDIUM
- verbosity: MEDIUM
</System>
```

user prompt:
```
User: 
EVIDENCE = [<JSON array of chunk objects, ordered by reranker score (best first)>]

USER_QUERY:
"Answer this question using ONLY the evidence above. Provide a concise answer (<= 600 tokens) that addresses the question with philosophical nuance. For each factual sentence include a parenthetical citation to the specific chunk id used. At the end include a 'used_evidence' array with each chunk id you relied on and the exact verbatim sentence(s) you copied from that chunk."

Formatting rules (must follow):
1) First output a short 1-2 sentence TL;DR labeled "answer".
2) Then provide a short bullet explanation supporting the TL;DR; each bullet must have inline citations.
3) Then output a JSON object named "used_evidence" listing {id, verbatim_quote, role_in_answer}.
4) End with "UNSUPPORTED CLAIMS:" â€” a bullet list of anything you asserted that wasn't supported by evidence.

Respond with only JSON and nothing else, following this schema:
```
{
  "answer": "string",
  "used_evidence": [
    {
      "id": "string",
      "verbatim_quote": "string",
      "role_in_answer": "string"
    }
  ]
}
```

Please answer in British English.

QUESTION: <user question here>
```

evidence format:
```
{
  "id": "propositional-logic/3.1/0",
  "doc_title": "Propositional Logic (Stanford Encyclopedia of Philosophy)",
  "section_id": "3.1",
  "section_heading": "Multi-valued logics",
  "chunk_index": 0,
  "text": "Full chunk text here (only include when budget permits)."
}
```