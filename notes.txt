1. fetch SEP list of articles - https://plato.stanford.edu/published.html
2. check changes - https://plato.stanford.edu/new.html or https://plato.stanford.edu/rss/sep.xml
3. fetch new articles and revised articles
4. parse articles & update metadata in db
5. chunk articles & save chunks + metadata
6. vectorise and normalise chunks, attach metadata, upload to Vectorize

- chunk first, then preprocess
- store two versions: cleaned (retrieval), rich (user display + LLM generation)

https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/rpc/
https://github.com/niieani/gpt-tokenizer

Chunk size: <= 1024 (semantic chunking)
BM25: b=0.85, k1=1.6

===

SEP Oracle

initial setup, per article:
1. fetch article
2. preprocess and chunk (two formats: retrieval & generation)
3. store generation format in R2
4. store retrieval format in D1
5. vectorise retrieval format chunks & store in Vectorize

to answer user query:
1. query Vectorize
2. embed query and search D1 with BM25
3. unify top-K with RRF and rerank
4. dedupe chunks then grab top 12 and send generation format to LLM

to do future:
- https://www.elastic.co/search-labs/blog/advanced-chunking-fetch-surrounding-chunks
- https://www.pinecone.io/learn/advanced-rag-techniques
- streamed responses
- store short section refs to allow deep linking